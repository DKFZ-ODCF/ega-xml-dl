#!/bin/bash

#set -eu

# user agent to report as; let the archive know who is hammering their servers
I_AM_THE_ONE_WHO_KNOCKS="DKFZ ega crawler - odcf-service@dkfz-heidelberg.de"


# count lines in a file, returning ONLY the numeric count 
linecount() {
  wc -l "$1" | cut -d' ' -f1
}

todo_for() {
    echo "$OUTPUT_DIR/todo-$1.txt"
}



# Login Information for EGA
# TODO: make sure cmd input is KeePass compatible (tab separated)

if [ -z $1 ]; then # see if box was specified on command line
  read -p "From which box do you wish to download? ega-box-" BOX_NR
  BOX="ega-box-$BOX_NR"
else
  BOX="$1"
fi
# get password
read -p "password for $BOX? (ctrl+shift+v to paste) " -s PASSWORD
echo 


# timestamp so we can 'version' our data
DATE=$(date '+%Y-%m-%d')

# where to put our batch downloads
OUTPUT_DIR="$HOME/ega-xml/$BOX";
echo "Outputting downloads to $OUTPUT_DIR"

if [ ! -d "$OUTPUT_DIR" ]; then mkdir -p "$OUTPUT_DIR"; fi;


###################################
# STEP 1: grab list of todo's

# for each of the raw data-types of EGA
# note, TYPE-strings match EGA-API as-is (including spelling errors), do not "fix", or you'll get errors
# skip 'project' and 'policys', as there is no content there via this format.
TYPES=('analyses' 'experiments' 'runs' 'samples' 'studies' 'submissions' 'dacs' 'datasets')
for TYPE in "${TYPES[@]}"; do
    echo "preparing todo-list for $TYPE"
    # construct the overview URL
    URL="https://www.ebi.ac.uk/ena/submit/report/$TYPE/?format=csv&max-results=99999"

    # unfortunately, there is no way to use recursive wget, because the EBI does not offer
    # any listing of all the XML files (even requesting the overview with "format=xml" does nothing
    #
    # curl: fetches the table of all available data
    # tail: skip the header line
    # cut: extract the column holding EGA-IDs
    # sed: convert EGA-ID into a fetchable XML URL
    curl \
        -u "$BOX:$PASSWORD" \
        --user-agent "$I_AM_THE_ONE_WHO_KNOCKS" \
        --compressed \
        --limit-rate 1M \
        --silent \
        --ssl-reqd \
        "$URL" \
      | tail -n+2 \
      | cut -d, -f2 \
      > "$( todo_for $TYPE )"
done

echo "done preparing ToDo lists, now getting files"

for TYPE in "${TYPES[@]}"; do
    TYPE_DIR="$OUTPUT_DIR/$TYPE"
    TODO="$( todo_for $TYPE )"

    # store all XML files per analysis type
    if [ ! -d "$TYPE_DIR" ]; then mkdir "$TYPE_DIR"; fi;
    cd "$TYPE_DIR"

    ###################################
	# STEP 2A: filter what we already have, if any

    echo "checking $TYPE for pre-existing downloads"
    echo "   unfiltered ToDo-list has $( linecount "$TODO" ) items"

    HAVE="$OUTPUT_DIR/already-have-$TYPE.txt"

    # look for files already having a complete closing marker
    #   because the EBI sends pretty-printed (indented) XML, 
    #   closing marker at start-of-line means that the file is complete
    grep --files-with-matches --recursive --include=EGA* --include=ER* --extended-regexp "^</.+>$" \
        | grep --only-matching --extended-regexp "(EGA|ER).[0-9]+" > "$HAVE"

    # remove all file-names in already-have from todo
    grep --invert-match --fixed-strings --file="$HAVE" "$TODO" > "$OUTPUT_DIR/tmp-$TYPE.txt"
    mv "$OUTPUT_DIR/tmp-$TYPE.txt" "$TODO"
    echo "   removed $( linecount "$HAVE" ) previously downloaded items, $( linecount "$TODO" ) remaining"
    rm "$HAVE"

    ###################################
    # STEP 2B: actually get stuff
    echo "processing $TODO ($( linecount "$TODO" ) items)"

    # get batch of XML files for this data-type
    while read ID; do
        URL="https://wwwdev.ebi.ac.uk/ena/submit/drop-box/$TYPE/$ID?format=xml%"
        curl \
            -u "$BOX:$PASSWORD" \
            --user-agent "$I_AM_THE_ONE_WHO_KNOCKS" \
            --compressed \
            --limit-rate 1M \
            --silent \
            --ssl-reqd \
            --output "$ID" \
            "$URL"
    done < "$TODO"
    rm "$TODO"
done

echo "DONE: got data for all types!"
